{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minseung2006/Sentiment-Analysis-Project/blob/main/Sentiment_Analysis_Model_Project(OPENQQUANTIFY).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_lYFzwWBJy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ffae0bd-ac4e-4242-8895-f61618dc2b9e"
      },
      "outputs": [],
      "source": [
        "print(\"Hello world\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Imports all the necessary libraries for the Sentiment Analysis Model.\n",
        "!pip install kaggle\n",
        "import re\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import files\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktw97GoZ-OyJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "6ce1feff-d9ce-4117-d8da-b244747e4d8e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-40c0cf72-cef9-4cd7-9e2f-84660849b2c4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-40c0cf72-cef9-4cd7-9e2f-84660849b2c4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/clovisdalmolinvieira/news-sentiment-analysis\n",
            "License(s): other\n",
            "Downloading news-sentiment-analysis.zip to /content\n",
            "  0% 0.00/655k [00:00<?, ?B/s]\n",
            "100% 655k/655k [00:00<00:00, 1.06GB/s]\n",
            "Archive:  news-sentiment-analysis.zip\n",
            "  inflating: news_sentiment_analysis.csv  \n"
          ]
        }
      ],
      "source": [
        "# # Unload our file.\n",
        "files.upload()\n",
        "\n",
        "# The basic syntax of setting up my kaggle API into my machine.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d clovisdalmolinvieira/news-sentiment-analysis\n",
        "!unzip news-sentiment-analysis.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXFE5MurQ3Wh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2aa58f6-bf9c-4c30-8217-541749602f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Source          Author  \\\n",
            "0        stgnews  Bridger Palmer   \n",
            "1  Zimbabwe Mail  Staff Reporter   \n",
            "2      4-traders             NaN   \n",
            "3      4-traders             NaN   \n",
            "4         PLANET             NaN   \n",
            "\n",
            "                                               Title  \\\n",
            "0  Pine View High teacher wins Best in State awar...   \n",
            "1  Businesses Face Financial Strain Amid Liquidit...   \n",
            "2  Musk donates to super pac working to elect Tru...   \n",
            "3  US FTC issues warning to franchisors over unfa...   \n",
            "4                          Rooftop solar's dark side   \n",
            "\n",
            "                                         Description  \\\n",
            "0  ST. GEORGE — Kaitlyn Larson, a first-year teac...   \n",
            "1  Harare, Zimbabwe – Local businesses are grappl...   \n",
            "2  (marketscreener.com) Billionaire Elon Musk has...   \n",
            "3  (marketscreener.com) A U.S. trade regulator on...   \n",
            "4  4.5 million households in the U.S. have solar ...   \n",
            "\n",
            "                                                 URL  \\\n",
            "0  https://www.stgeorgeutah.com/news/archive/2024...   \n",
            "1  https://www.thezimbabwemail.com/business/busin...   \n",
            "2  https://www.marketscreener.com/business-leader...   \n",
            "3  https://www.marketscreener.com/quote/stock/MCD...   \n",
            "4  https://www.npr.org/2024/07/12/1197961036/roof...   \n",
            "\n",
            "                Published At Sentiment      Type  \n",
            "0  2024-07-12T23:45:25+00:00  positive  Business  \n",
            "1  2024-07-12T22:59:42+00:00   neutral  Business  \n",
            "2  2024-07-12T22:52:55+00:00  positive  Business  \n",
            "3  2024-07-12T22:41:01+00:00  negative  Business  \n",
            "4  2024-07-12T22:28:19+00:00  positive  Business  \n"
          ]
        }
      ],
      "source": [
        "# Extract our dataset to the Google Colab.\n",
        "extracted_file_path = '/content/news_sentiment_analysis.csv'\n",
        "df = pd.read_csv(extracted_file_path)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iJe50HORCdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba40fc9-9e7f-499b-9142-8958903a1bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment\n",
            "positive    2134\n",
            "neutral      789\n",
            "negative     577\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Drop rows with missing values based on the description column.\n",
        "df = df.dropna(subset=['Description', 'Sentiment'])\n",
        "\n",
        "# Ensure consistent labels\n",
        "df['Sentiment'] = df['Sentiment'].str.lower()  # Convert sentiments to lowercase\n",
        "print(df['Sentiment'].value_counts())  # Check sentiment distribution\n",
        "\n",
        "# Clean text data with a function clean_text.\n",
        "def clean_text(text):\n",
        "    # Through the text argument of the clean_text function, we remove any urls, mentions, hashtags,\n",
        "    # and leading and trailing whitespaces respectively.\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# We are applying the description column with the clean_text function.\n",
        "df['Description'] = df['Description'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1aRZSWtRGje",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "9040c5e5495949d1bf823b685dc342f1",
            "bcbf2508b33d43e1b46ec657d01a3244",
            "09e4fa7e841f4e04a0236f0b7fa944f4",
            "b003c80b3f3a4b968883e3637d088f8a",
            "19f9d56a7d214ced858630fd6f229911",
            "288b1d85e59a49fabcc2e9fa6cf073be",
            "40aa21c4fafa428f987491f3222a608b",
            "dd2b375fb53449368746c08e7600f539",
            "35c96990a266465f8b2ed10b325dedd4",
            "1011f0e2107b493f8122f1d372fbb758",
            "071b8df5b97847229be6f87cebfbee30",
            "31ea33519b2c418ab5e3414cc7558c9f",
            "5283f00df68f432b9f31f7e33e09563c",
            "7c15fc15a50f4b83b6cbf02a4ab6c403",
            "b65b90112eb842cab2d768631a501269",
            "f861883b8f1142c5ab55aeeabd9080db",
            "c1711fb3ca414f53a58ef6215648c833",
            "1eca24345b5b42caa9dd40c1582cd5f3",
            "579d047047b7423ba37935a4b4801b12",
            "ae8d7960d1c64ff5898560d151e0e769",
            "0f59274b310c4de39d454c72550d56ed",
            "eb01365f7ca84aa4844a8ebe4a2bef59",
            "6be983d6608a4dcd87d064722e546fda",
            "9156bb0ddca24f4b9dbf7449367dc88c",
            "e25782ad7b3f4f918a712c1abc127a52",
            "812c8e3aa12a45eeab11d75e839dfa85",
            "6e27d018ff2c42a9b2f6691372dff285",
            "a7b8c934f8144dfd87040cd9717dd9c8",
            "8c2945e830d1488ead166b32ef9ab3a9",
            "ea747470108d4311b13b39f752f7ea8b",
            "eb5e8bfcdd074a93b50a5bba27460b0c",
            "9a9eb4b434ef4126af67994218b8bdf0",
            "509103188aed46dc8262358dfcff41f4",
            "3cc93f39d0ba477484ced9e6fe75059a",
            "68d59dff74874535bb8cc022dfc23584",
            "604a5d74cb554223aeab85c77e1bfd94",
            "c3844798c7514a27916c2a5cbb527b56",
            "56f021c6c4ed45d59b3c7328d083e199",
            "13597b988efc4950b64c3750a74e5333",
            "502440c76fd54f94ad8154818f4de2a6",
            "4204743f01e2410aa5f72043c049f8f8",
            "50799cd0c884473b9ac21900abebdd84",
            "8e02df47d6b44e95b24ebe850d6d5c0a",
            "b5dcc5649c7545d6885b169a62395d31",
            "3037e024e1f940348909f879e2ddc8ea",
            "f140893282bb4affb9733990e2ceaa61",
            "fc9fa7a03cfc4dbea05fbea5d808230c",
            "f5fca5addc61470e9bdbaf0f566c3154",
            "fce6a05fcef2406cbbb751d79a97e4a9",
            "3f7eb2a0ebcf46b8ac6db85251f0f288",
            "187be62d078c411d89eac9fc2bbb5773",
            "46a7b0e655844bfeaa4f5b85063747e3",
            "aa2b292eccbd472a8c0e94973174c0c0",
            "0d65ff7da6094a7e8a437e61f61eb24b",
            "3425949fe0ec47829ffc9018349f04a5"
          ]
        },
        "outputId": "78141aa0-70c5-460d-ec3f-07fec237c016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9040c5e5495949d1bf823b685dc342f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31ea33519b2c418ab5e3414cc7558c9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6be983d6608a4dcd87d064722e546fda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cc93f39d0ba477484ced9e6fe75059a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3037e024e1f940348909f879e2ddc8ea"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# START OF THE MACHINE LEARNING PROCESS\n",
        "# Initialize tokenizer and the max length.\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "MAX_LEN = 512\n",
        "\n",
        "# Tokenize data in a tokenize_data function. With the initialization of input_ids and attention_masks.\n",
        "def tokenize_data(data, tokenizer, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # Iterate each text in the data with the following parameters in the encoded variable.\n",
        "    for text in data:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            # The following parameters are necessary components to scan through each text in the data.\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        # Apply the input_ids and attention_masks with the encoded variable with all the respective parameters.\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    # Use the torch library to then use the input_ids and the attention_masks.\n",
        "    # The input_ids are a PyTorch tensor that stores numerical identifiers for each token in the text dataset.\n",
        "    # The attention_masks are a way to do necessary padding of the text in the dataset to make them an appropriate length. (Ignore the padded tokens of the data).\n",
        "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# Using the input_ids and attention_masks variables, this calls the tokenize_data function and applies the Description column to run the tokenizer.\n",
        "input_ids, attention_masks = tokenize_data(df['Description'], tokenizer, MAX_LEN)\n",
        "\n",
        "# Encode labels with label_encoder.\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(df['Sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRO9Zk_uRMTf"
      },
      "outputs": [],
      "source": [
        "# Split data with the appropriate test/validation size, random state, the input_ids and attention_masks we put in earlier.\n",
        "train_inputs, val_test_inputs, train_masks, val_test_masks, train_labels, val_test_labels = train_test_split(\n",
        "    input_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Splits the test/validation data into validation and test data. Splitting it in half gives 10% of total data to validation and 10% for testing.\n",
        "val_inputs, test_inputs, val_masks, test_masks, val_labels, test_labels = train_test_split(\n",
        "    val_test_inputs, val_test_masks, val_test_labels, test_size=.5, random_state=42\n",
        ")\n",
        "\n",
        "# Create PyTorch Datasets. The first one creates a TensorDataset and combine the train_inputs, which are tokenized inputs,\n",
        "# train_masks for training, indicating which tokens should be attended to, and torch.tensor which correspond sentiment labels covert to PyTorch.\n",
        "train_dataset = TensorDataset(train_inputs, train_masks, torch.tensor(train_labels))\n",
        "\n",
        "# This second one does the same thing as the above one but this is the validation data.\n",
        "val_dataset = TensorDataset(val_inputs, val_masks, torch.tensor(val_labels))\n",
        "\n",
        "# This third does the same as above, but with test data.\n",
        "test_dataset = TensorDataset(test_inputs, test_masks, torch.tensor(test_labels))\n",
        "\n",
        "# Create DataLoaders to train and load the data using train_dataset, val_dataset, and test_dataset.\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae6ZfvkbROum",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "032bc16f185e409aa0d47ae8b0d32158",
            "51aeb1831a00440c8d275d01bb86de90",
            "83d60e64a6a044eb9c9451fb76920ca9",
            "1ae832cbf122446581f744db04cf896d",
            "acd82e108968462ebc70e99587e46988",
            "3a6a0a48a88b49e09d1781ee693debc4",
            "6b2b3ae12ab645c18041cf5792ce58b9",
            "e71fb8cca3de48f38ee4f270a204e4c6",
            "da87426063874a9cbf0cd479d4c1527c",
            "aacb0afeb6374f4bb1d63f5ebf70fe55",
            "1f8ffaf5d6f8443ea77316a9a64f1f1b"
          ]
        },
        "outputId": "db30f16b-924e-4cda-b8e5-4c31fe23bfe1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "032bc16f185e409aa0d47ae8b0d32158"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Loads the model.\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Define optimizer initialization.\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5Nrfk82RQmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0ff584-b439-4f66-e84d-7b62f256468c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8-2988678322.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-8-2988678322.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed.\n",
            "Epoch 2 completed.\n",
            "Epoch 3 completed.\n"
          ]
        }
      ],
      "source": [
        "# Check device and remaining cpu.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the appropriate device.\n",
        "model.to(device)\n",
        "\n",
        "# Reduce the scale steps and reduce the process time.\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(3):  # Adjust the number of epochs as needed to 3.\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move all batch inputs to the same device\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_masks = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass using the input_ids and attention_masks.\n",
        "        with autocast():\n",
        "            outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        # Backward pass by the GradScaler.\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    # Double check if one loop is completed.\n",
        "    print(f\"Epoch {epoch + 1} completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPKtPnlYUEtd"
      },
      "outputs": [],
      "source": [
        "# Validation loop\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "for batch in val_dataloader:\n",
        "    # Move all batch inputs to the same device.\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_masks = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "\n",
        "    # Disable gradient calculations and has an output variable that has the model with the input_ids and attention_masks.\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_masks)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Move predictions and labels to the CPU for evaluation. This uses the numpy library for the main function.\n",
        "    predictions.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb-oKNrOUMdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b28a046-715d-42ef-ad70-55bcce698d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.81      0.52      0.64        65\n",
            "     neutral       0.76      0.86      0.81        78\n",
            "    positive       0.85      0.90      0.88       207\n",
            "\n",
            "    accuracy                           0.82       350\n",
            "   macro avg       0.81      0.76      0.77       350\n",
            "weighted avg       0.82      0.82      0.82       350\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Classification report that will be used for the Hyperparameter tuning process.\n",
        "print(classification_report(true_labels, predictions, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing google drive for importing/exporting to permanently save models and hyper parameter settings:\n",
        "from google.colab import drive\n",
        "\n",
        "# Specifying location to put google drive files in for google collab\n",
        "file_location = '/content/drive/'\n",
        "\n",
        "# Specifying location for the model to go into your google drive\n",
        "model_location = file_location + 'MyDrive/Sentiment_Analysis_Model/Model'\n",
        "hyperparameter_location = file_location + 'MyDrive/Sentiment_Analysis_Model/HyperParameters'\n",
        "\n",
        "# Mounting the specified location for google drive.\n",
        "# Note: It will access your drive, but the files will be deleted after the session ends.\n",
        "drive.mount(file_location)\n"
      ],
      "metadata": {
        "id": "bMxCByFSyHLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa43dd62-d6e9-4310-8513-d8506197b0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining general functions that will be used in validation testing:\n",
        "# Defining Functions:\n",
        "\n",
        "# Training Loop(kind of similar to the one with the machine learning process\n",
        "# Using the model, data_loader, and optimizar parameter for this function.\n",
        "# The train_one_epoch function reduces redundancy.\n",
        "def train_one_epoch(model, data_loader, optimizer):\n",
        "    # Reduce the scale steps and reduce the process time.\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        # Using the input_ids and attention_masks from before and set it equal to each item in the batch of data_loader.\n",
        "        input_ids, attention_masks, labels = (item.to(device) for item in batch)\n",
        "\n",
        "        # Forward pass again with the input_ids and the attention_masks.\n",
        "        with autocast():\n",
        "          outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
        "          loss = outputs.loss\n",
        "\n",
        "        # Backward pass using the GradScaler.\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Store the model variable with the train_one_epoch function with the same parameters except it's the train_dataloader.\n",
        "# Changed it to accurately reflect the amount of epochs the trial is suggesting for more accuracy. (This is used for the epoch in the training dataset.)\n",
        "def train_model(model, train_dataloader, optimizer, num_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    model = train_one_epoch(model, train_dataloader, optimizer)\n",
        "    print(f\"Completed Epoch {epoch + 1}.\")\n",
        "  return model\n",
        "\n",
        "# Evaluates the model to get the F1 score. Uses the F1 score to determine which model had the best parameters. (This is displayed for every epoch set.)\n",
        "def eval_model(model, val_dataloader):\n",
        "  model.eval()\n",
        "  predictions, true_labels = [], []\n",
        "  # Iterates throught the batch in the dataloader and moves to the batch to the same device.\n",
        "  for batch in val_dataloader:\n",
        "      input_ids = batch[0].to(device)\n",
        "      attention_masks = batch[1].to(device)\n",
        "      labels = batch[2].to(device)\n",
        "      # Creates a predicting set of outcomes from the input_ids and attention_masks.\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input_ids, attention_mask=attention_masks)\n",
        "      logits = outputs.logits\n",
        "      # Move predictions and labels to the CPU for evaluation. This uses the numpy library for the main function.\n",
        "      predictions.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
        "      true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  # Calculates and returns F1-score after the epoch set was executed.\n",
        "  report = classification_report(true_labels, predictions, output_dict=True, target_names=label_encoder.classes_)\n",
        "  f1 = report['weighted avg']['f1-score']\n",
        "  print(f\"F1 Score for this model: {f1}\")\n",
        "  return f1\n",
        "\n",
        "# Freeing memory for extra optimization.\n",
        "def clean_memory(model, optimizer):\n",
        "  del model, optimizer\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "vTpK-PBYL_A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuxeuKuGi497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "720f22d72e3e42eaa98ff85c65a688dc",
            "5687325343da41b2b301c291496e2620",
            "d24e825f68944159a9152045d480d8b9",
            "6c404ff34aee46838af7b0bcc2b9d9ea",
            "da08e5e6fbb34df2ac3767a60cef14c3",
            "aa0f3c4b1ad84479ae6f758de1ae98c3",
            "b40e48b77c7c4a56ac22f0fe88405ed0",
            "9cfc6020a79e4902bd04c76819cf7b47",
            "efbcd92ea9ad4b65be046f0870e9732c",
            "a81323aede684384be8c9b85cf2dab92",
            "3d14b5a660ce4c38817845f45ea00af3"
          ]
        },
        "outputId": "5732025b-7b5a-4b53-fb18-9b8c00384355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.2 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-08 01:42:25,284] A new study created in memory with name: no-name-1d9fb56a-f10e-476e-9311-78a18a3810cf\n",
            "/tmp/ipython-input-8-2359829646.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
            "/tmp/ipython-input-8-2359829646.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "720f22d72e3e42eaa98ff85c65a688dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.8353872566103925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.799091461599247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.7891073455254046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.800111817199025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.7637570857045008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-08 01:55:46,759] Trial 0 finished with value: 0.797490993327714 and parameters: {'learning_rate': 4.803691007947765e-05, 'batch_size': 16, 'num_epochs': 2}. Best is trial 0 with value: 0.797490993327714.\n",
            "/tmp/ipython-input-8-2359829646.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
            "/tmp/ipython-input-8-2359829646.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8494698515769944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8445161893577549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8709711648062398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8441910888468427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8052904873706752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-08 02:23:48,011] Trial 1 finished with value: 0.8428877563917012 and parameters: {'learning_rate': 1.3119263040420867e-05, 'batch_size': 8, 'num_epochs': 4}. Best is trial 1 with value: 0.8428877563917012.\n",
            "/tmp/ipython-input-8-2359829646.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
            "/tmp/ipython-input-8-2359829646.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "F1 Score for this model: 0.8373245138517583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "F1 Score for this model: 0.8451104436111546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "F1 Score for this model: 0.8555056592772856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "F1 Score for this model: 0.8184114745144543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "F1 Score for this model: 0.8141738878633665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-08 02:41:28,380] Trial 2 finished with value: 0.8341051958236039 and parameters: {'learning_rate': 2.955560633247276e-05, 'batch_size': 32, 'num_epochs': 3}. Best is trial 1 with value: 0.8428877563917012.\n",
            "/tmp/ipython-input-8-2359829646.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
            "/tmp/ipython-input-8-2359829646.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8809165949131952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8491754298049495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score for this model: 0.46223602484472043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.854064165816958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8357982746071363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-08 03:09:26,303] Trial 3 finished with value: 0.7764380979973919 and parameters: {'learning_rate': 4.875925956489517e-05, 'batch_size': 8, 'num_epochs': 4}. Best is trial 1 with value: 0.8428877563917012.\n",
            "/tmp/ipython-input-8-2359829646.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
            "/tmp/ipython-input-8-2359829646.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.8289041537728781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.793677694539144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.8174222927591319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.8092280406130038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-8-2359829646.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "F1 Score for this model: 0.8072631094447938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-08 03:24:13,835] Trial 4 finished with value: 0.8112990582257902 and parameters: {'learning_rate': 1.8423941154727783e-05, 'batch_size': 8, 'num_epochs': 2}. Best is trial 1 with value: 0.8428877563917012.\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter tuning\n",
        "# Installs the Optuna library for doing Hyperparameter learning process.\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import gc\n",
        "from functools import partial\n",
        "\n",
        "# Define the objective function for Optuna with a trial parameter.\n",
        "def objective(trial, input_ids, attention_masks, labels):\n",
        "\n",
        "    # Suggest hyperparameters for the learning rate, batch size, and number of epochs variables.\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
        "    num_epochs = trial.suggest_int('num_epochs', 2, 4)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Splitting the data using Stratified K-Fold for validation, using 5 splits(using 2 for testing)\n",
        "    # Splits data into 5 parts, one used for testing while the other 4 used for training\n",
        "    skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 42)\n",
        "\n",
        "    # Alternates which part is used for testing until each part has been used once for testing\n",
        "    # Returns mean of all f1 scores to determine which one best generalizes the data from validation.\n",
        "    f1_scores_list = []\n",
        "\n",
        "    for training_index, testing_index in skf.split(input_ids, labels):\n",
        "\n",
        "      # Initialize model and optimizer by using the classification_report from the previous code cell.\n",
        "      model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_))\n",
        "      model.to(device)\n",
        "      optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      # Seperates the data using the K-Fold Intervals. Each k set fold is a validation set in return.\n",
        "      training_inputs  = input_ids[training_index]\n",
        "      training_masks = attention_masks[training_index]\n",
        "      testing_inputs = input_ids[testing_index]\n",
        "      testing_masks = attention_masks[testing_index]\n",
        "      training_labels = testing_labels = labels[training_index]\n",
        "      testing_labels = labels[testing_index]\n",
        "\n",
        "      # Creates the dataset used for dataloaders. We use the training_inputs and training_masks from the k-fold intervals.\n",
        "      train_dataset = TensorDataset(training_inputs, training_masks, torch.tensor(training_labels))\n",
        "      val_dataset = TensorDataset(testing_inputs, testing_masks, torch.tensor(testing_labels))\n",
        "\n",
        "      # Creates the dataloaders needed.\n",
        "      train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "      val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "      # Trains the model fully.\n",
        "      model = train_model(model, train_dataloader, optimizer, num_epochs)\n",
        "\n",
        "      # Evaluates model and adds it to the list with the f1 score.\n",
        "      f1_score = eval_model(model, val_dataloader)\n",
        "      f1_scores_list.append(f1_score)\n",
        "\n",
        "      # Cleans memory for the next data training modeling we're doing.\n",
        "      clean_memory(model, optimizer)\n",
        "\n",
        "    # Averages the F1 Scores, returns for final evaluation.\n",
        "    sum = 0\n",
        "    for i in f1_scores_list:\n",
        "      sum += i\n",
        "\n",
        "    return sum/len(f1_scores_list)\n",
        "\n",
        "# Redoing the data process in order to re-align the data for K-Fold. All of this is based on the Sentiment and Description. (Those are the two columns of data we are working with.)\n",
        "input_ids, attention_masks = tokenize_data(df['Description'], tokenizer, MAX_LEN)\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(df['Sentiment'])\n",
        "\n",
        "check_study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "check_study.optimize(partial(objective, input_ids=input_ids, attention_masks=attention_masks, labels=labels), n_trials=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the results of the hyperparameter tuning, we apply bootstrapping validation method.\n",
        "\n",
        "# Bootstrapping Process\n",
        "bootstrapping_iteration = 15\n",
        "f1_scores = []\n",
        "\n",
        "# The most optimal way for bootstrapping. Take the best hyperparameters, train a final model, and use bootstrapping to finalize the validations.\n",
        "# There is another more complex way(revise if possible and/or needed).\n",
        "best_params = check_study.best_trial.params\n",
        "\n",
        "# Double checks device for model training.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Implement a for loop that is taking the bootstrapping iterations.\n",
        "for i in range(bootstrapping_iteration):\n",
        "  # Takes the data randomly. Same data could be repeated for testing generalization in validation.\n",
        "  indices = np.random.choice(len(input_ids), size = len(input_ids), replace = True)\n",
        "  bootstrap_inputs = input_ids[indices]\n",
        "  bootstrap_masks = attention_masks[indices]\n",
        "  bootstrap_labels = labels[indices]\n",
        "\n",
        "  # Splits the data for validation testing and training.\n",
        "  train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
        "  bootstrap_inputs, bootstrap_masks, bootstrap_labels, test_size=.2)\n",
        "\n",
        "  # Trains the model so in each iteration, the model will train the set and go through the train test split process which we declared above.\n",
        "  model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_))\n",
        "  model.to(device)\n",
        "  optimizer = AdamW(model.parameters(), lr=best_params['learning_rate'])\n",
        "  train_dataset = TensorDataset(train_inputs, train_masks, torch.tensor(train_labels))\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
        "  model = train_model(model, train_dataloader, optimizer, best_params['num_epochs'])\n",
        "\n",
        "  # Then this gets the test result of the data during the iteration.\n",
        "  val_dataset = TensorDataset(val_inputs, val_masks, torch.tensor(val_labels))\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=best_params['batch_size'])\n",
        "  f1_score = eval_model(model, val_dataloader)\n",
        "  f1_scores.append(f1_score)\n",
        "\n",
        "  # Cleans memory for the next data training modeling we're doing.\n",
        "  clean_memory(model, optimizer)\n",
        "\n",
        "f1_validation = 0\n",
        "for f1_score in f1_scores:\n",
        "  f1_validation += f1_score\n",
        "\n",
        "f1_validation = f1_validation / len(f1_scores)\n",
        "print(f\"Average F1 Score: {f1_validation}\")\n"
      ],
      "metadata": {
        "id": "IzN0XfWBXhxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64e47e6-77b7-4744-e268-c8547bc6002d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8889996385237834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8845209187194597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8973359406901934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.897922426626824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.9218576650179681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.870809753853232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.9083105473050581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.9094607952753647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.9117684123500178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.9086768478722823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.9055253287114319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8934417848469708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.9048160062015703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.942457709823299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-25-3322292869.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-25-3322292869.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Epoch 1.\n",
            "Completed Epoch 2.\n",
            "Completed Epoch 3.\n",
            "Completed Epoch 4.\n",
            "F1 Score for this model: 0.8908776894180451\n",
            "Average F1 Score: 0.9024520976823669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bum4yHGhIsku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "767ccfb4-be2e-4467-f121-fdda934bec59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters from Optuna: {'learning_rate': 1.3119263040420867e-05, 'batch_size': 8, 'num_epochs': 4}\n",
            "Best Hyperparameters saved: {'learning_rate': 3.974084191654577e-05, 'batch_size': 8, 'num_epochs': 4, 'F1_Score': 0.8722440376170837}\n"
          ]
        }
      ],
      "source": [
        "# Import the os library to interact with the user's operating system and the json library to store the files and the drive.\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Best hyperparameters and finds the best one.\n",
        "print(\"Best Hyperparameters from Optuna:\", check_study.best_params)\n",
        "\n",
        "best_params = check_study.best_params\n",
        "\n",
        "# Checks current hyperparameters saved.\n",
        "if os.path.exists(hyperparameter_location):\n",
        "\n",
        "  # Opens read/write for where the hyper parameters are stored.\n",
        "  with open(hyperparameter_location, 'r+') as g_parameters:\n",
        "\n",
        "    # Loads settings as a JSON.\n",
        "    settings = json.load(g_parameters)\n",
        "    print(f'Best Hyperparameters saved: {settings}')\n",
        "\n",
        "    # Checks which hyper parameter has the better F1_Score, stores the better one in the GDrive.\n",
        "    if settings['F1_Score'] < f1_validation:\n",
        "      new_parameter = dict(check_study.best_params)\n",
        "      new_parameter['F1_Score'] = f1_validation\n",
        "      g_parameters.seek(0)\n",
        "      g_parameters.truncate()\n",
        "      g_parameters.write(json.dumps(new_parameter))\n",
        "\n",
        "    else:\n",
        "      # If google drive parmeters are better, uses those instead of Optuna\n",
        "      best_params = settings\n",
        "\n",
        "# If nothing is saved, creates a new location in the google drive and stores the settings.\n",
        "else:\n",
        "  os.makedirs(os.path.dirname(hyperparameter_location), exist_ok=True)\n",
        "  with open(hyperparameter_location, 'x') as g_parameters:\n",
        "    new_parameter = dict(check_study.best_params)\n",
        "    new_parameter['F1_Score'] = f1_validation\n",
        "    g_parameters.write(json.dumps(new_parameter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXPSYXv-8Z4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afefbbc-4f89-4a20-883c-ae15f7e8ca41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Check device and remaining cpu.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# This section is used again to make sure maximum optimization and to make sure no other erros occur in the training and data testing.\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_))\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
        "\n",
        "\n",
        "for epoch in range(best_params['num_epochs']):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_masks = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# Save the model for best measure.\n",
        "model.save_pretrained(\"best_roberta_model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Test, using the test data instead of validation data for generalization. (Validation set after Hyperparameter tuning.)\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "for batch in test_dataloader:\n",
        "    # Move all batch inputs to the same device.\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_masks = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "\n",
        "    # Disable gradient calculations and has an output variable that has the model with the input_ids and attention_masks.\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_masks)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Move predictions and labels to the CPU for evaluation\n",
        "    predictions.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Printing out the results\n",
        "print(classification_report(true_labels, predictions, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPim1tYnpkwJ",
        "outputId": "c2683405-9859-403c-83ae-266621694e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.81      0.83      0.82        66\n",
            "     neutral       0.79      0.96      0.86        69\n",
            "    positive       0.95      0.88      0.92       215\n",
            "\n",
            "    accuracy                           0.89       350\n",
            "   macro avg       0.85      0.89      0.87       350\n",
            "weighted avg       0.89      0.89      0.89       350\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If model performance is satisfactory, permanently save into google drive for exporting later:\n",
        "\n",
        "# Saving the model\n",
        "model.save_pretrained(model_location)"
      ],
      "metadata": {
        "id": "1Y14QlR1uHmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing model from google drive\n",
        "\n",
        "# Importing model:\n",
        "imported_model = RobertaForSequenceClassification.from_pretrained(model_location)"
      ],
      "metadata": {
        "id": "Ozp1CWpPwl3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imported model performance:\n",
        "\n",
        "# Moving to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "imported_model.to(device)\n",
        "\n",
        "# Using test data to show performance:\n",
        "imported_model.eval()\n",
        "predictions, true_labels = [], []\n",
        "for batch in test_dataloader:\n",
        "    # Move all batch inputs to the same device\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_masks = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "\n",
        "    # Disable gradient calculations and has an output variable that has the imported_model with the input_ids and attention_masks.\n",
        "    with torch.no_grad():\n",
        "        outputs = imported_model(input_ids, attention_mask=attention_masks)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Move predictions and labels to the CPU for evaluation.\n",
        "    predictions.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Printing out the final results for our ENITRE project.\n",
        "print(classification_report(true_labels, predictions, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGuphdi-zMaS",
        "outputId": "4588a607-b016-4266-8823-c1cff63054da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.81      0.83      0.82        66\n",
            "     neutral       0.79      0.96      0.86        69\n",
            "    positive       0.95      0.88      0.92       215\n",
            "\n",
            "    accuracy                           0.89       350\n",
            "   macro avg       0.85      0.89      0.87       350\n",
            "weighted avg       0.89      0.89      0.89       350\n",
            "\n"
          ]
        }
      ]
    }
  ]
